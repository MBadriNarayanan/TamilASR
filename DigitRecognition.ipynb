{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DigitRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6mfodhh4eW4",
        "outputId": "34c27660-b20f-4be0-8a15-13f29040662c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wSbhNzIN4i1P",
        "outputId": "9951d65b-f25d-4df5-87fb-6c1057ef81ef"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/deepspeech\n",
        "%cd /content/drive/My\\ Drive/deepspeech\n",
        "\n",
        "!pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3\n",
        "!pip3 install --upgrade --force-reinstall -e .\n",
        "!python util/taskcluster.py --arch gpu --target tc/ --branch v0.7.4\n",
        "\n",
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.8\n",
        "\n",
        "# Uninstall existing tf and install the correct\n",
        "!pip uninstall -y tensorflow\n",
        "!pip uninstall -y tensorflow-gpu\n",
        "%tensorflow_version 1.x\n",
        "!pip install tensorflow-gpu==1.15.2\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech\n",
            "/content/drive/My Drive/deepspeech\n",
            "Collecting pip==20.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 9.1MB/s \n",
            "\u001b[?25hCollecting wheel==0.34.2\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
            "Collecting setuptools==46.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/df/635cdb901ee4a8a42ec68e480c49f85f4c59e8816effbf57d9e6ee8b3588/setuptools-46.1.3-py3-none-any.whl (582kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 17.3MB/s \n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement wheel~=0.35, but you'll have wheel 0.34.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip, wheel, setuptools\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "  Found existing installation: wheel 0.36.2\n",
            "    Uninstalling wheel-0.36.2:\n",
            "      Successfully uninstalled wheel-0.36.2\n",
            "  Found existing installation: setuptools 56.0.0\n",
            "    Uninstalling setuptools-56.0.0:\n",
            "      Successfully uninstalled setuptools-56.0.0\n",
            "Successfully installed pip-20.0.2 setuptools-46.1.3 wheel-0.34.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/My%20Drive/deepspeech\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 176 kB/s \n",
            "\u001b[?25hCollecting progressbar2\n",
            "  Downloading progressbar2-3.53.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pyxdg\n",
            "  Downloading pyxdg-0.27-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting absl-py\n",
            "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting opuslib==2.0.0\n",
            "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.7.0-py3-none-any.whl (293 kB)\n",
            "\u001b[K     |████████████████████████████████| 293 kB 62.7 MB/s \n",
            "\u001b[?25hCollecting sox\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 63.4 MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting numba==0.47.0\n",
            "  Downloading numba-0.47.0-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 69.0 MB/s \n",
            "\u001b[?25hCollecting llvmlite==0.31.0\n",
            "  Downloading llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting librosa\n",
            "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 76.5 MB/s \n",
            "\u001b[?25hCollecting soundfile\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting ds_ctcdecoder==0.7.4\n",
            "  Downloading ds_ctcdecoder-0.7.4-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 57.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.2\n",
            "  Downloading tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 40 kB/s \n",
            "\u001b[?25hCollecting python-utils>=2.3.0\n",
            "  Downloading python_utils-2.5.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting sqlalchemy>=1.1.0\n",
            "  Downloading SQLAlchemy-1.4.11-cp37-cp37m-manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 63.2 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting scipy!=1.4.0\n",
            "  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 64 kB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "  Downloading alembic-1.5.8-py2.py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 80.4 MB/s \n",
            "\u001b[?25hCollecting cliff\n",
            "  Downloading cliff-3.7.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.6 MB/s \n",
            "\u001b[?25hCollecting colorlog\n",
            "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 71.7 MB/s \n",
            "\u001b[?25hCollecting pytz>=2017.3\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 68.3 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.7.3\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting chardet<5,>=3.0.2\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 78.7 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 76.8 MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Downloading setuptools-56.0.0-py3-none-any.whl (784 kB)\n",
            "\u001b[K     |████████████████████████████████| 784 kB 49.7 MB/s \n",
            "\u001b[?25hCollecting audioread>=2.0.0\n",
            "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
            "\u001b[K     |████████████████████████████████| 377 kB 73.5 MB/s \n",
            "\u001b[?25hCollecting scikit-learn!=0.19.0,>=0.14.0\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 98 kB/s \n",
            "\u001b[?25hCollecting joblib>=0.14\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 64.1 MB/s \n",
            "\u001b[?25hCollecting decorator>=3.0.0\n",
            "  Downloading decorator-5.0.7-py3-none-any.whl (8.8 kB)\n",
            "Collecting resampy>=0.2.2\n",
            "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
            "\u001b[K     |████████████████████████████████| 323 kB 75.4 MB/s \n",
            "\u001b[?25hCollecting pooch>=1.0\n",
            "  Downloading pooch-1.3.0-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 582 kB/s \n",
            "\u001b[?25hCollecting cffi>=1.0\n",
            "  Downloading cffi-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting wheel>=0.26; python_version >= \"3\"\n",
            "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting protobuf>=3.6.1\n",
            "  Downloading protobuf-3.15.8-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 59.4 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 62.1 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 361 kB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.37.0-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 46.5 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\"\n",
            "  Downloading importlib_metadata-4.0.1-py3-none-any.whl (16 kB)\n",
            "Collecting greenlet!=0.4.17; python_version >= \"3\"\n",
            "  Downloading greenlet-1.0.0-cp37-cp37m-manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 75.7 MB/s \n",
            "\u001b[?25hCollecting pyparsing>=2.0.2\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.5.1-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-1.5.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 75.5 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=3.12\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 68.6 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting PrettyTable>=0.7.2\n",
            "  Downloading prettytable-2.1.0-py3-none-any.whl (22 kB)\n",
            "Collecting soupsieve>1.2; python_version >= \"3.0\"\n",
            "  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting appdirs\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting pycparser\n",
            "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 78.5 MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 79.9 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 62.9 MB/s \n",
            "\u001b[?25hCollecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
            "Collecting MarkupSafe>=0.9.2\n",
            "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux2010_x86_64.whl (33 kB)\n",
            "Collecting wcwidth>=0.1.7\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Collecting attrs>=16.3.0\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting cached-property; python_version < \"3.8\"\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Building wheels for collected packages: opuslib, bs4, librosa, audioread, resampy, termcolor, gast, wrapt, pyperclip\n",
            "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=ade8ba2f87f2379fb224ae465632fdcfbf1510106c431abe3a3dc472174d3245\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/ba/d4/0e81231a9797fbb262ae3a54fd761fab850db7f32d94a3283a\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=06560fa72a801d15ab5a2b95d96a2a8c457cbd45e49567df46669f4d409a4198\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/9e/ba/20e5bbc1afef3a491f0b3bb74d508f99403aabe76eda2167ca\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201374 sha256=0a76314d813ebb116ce5065f17589a3036604dbf71ab6bd2319a1860b4a4bd10\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/1e/aa/d91797ae7e1ce11853ee100bee9d1781ae9d750e7458c95afb\n",
            "  Building wheel for audioread (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for audioread: filename=audioread-2.1.9-py3-none-any.whl size=23142 sha256=ffa15c70d28808bf3209c4e23cc400addf7ca427940c548c2dda5e2293968373\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/7b/eb/213741ccc0678f63e346ab8dff10495995ca3f426af87b8d88\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320720 sha256=4212719f1f5057f366c6faa936a25ed628b91be51d19038c8733b19fe9571a71\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/18/0a/8ad18a597d8333a142c9789338a96a6208f1198d290ece356c\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=35789f7b46a0a9ee210c7c311c7f2758ddb88cd1eb801a46745e0c4bd496407f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=6aa68b6c2535bdb28528b9c99d3146852f5f9dbe5f173aab562825583caf52ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68673 sha256=4d435105cb6ede8154f2ea3586f5720dad38f009f12278e71451df43418252dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11107 sha256=3d47b57b3b6b1949bb71965f88335e7d817e165658a3c4e7d1ca25235475f9a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built opuslib bs4 librosa audioread resampy termcolor gast wrapt pyperclip\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: networkx 2.5.1 has requirement decorator<5,>=4.3, but you'll have decorator 5.0.7 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: moviepy 0.2.3.5 has requirement decorator<5.0,>=4.0.2, but you'll have decorator 5.0.7 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, six, python-utils, progressbar2, pyxdg, attrdict, absl-py, semver, opuslib, cmaes, typing-extensions, zipp, importlib-metadata, greenlet, sqlalchemy, tqdm, pyparsing, packaging, scipy, python-editor, python-dateutil, MarkupSafe, Mako, alembic, pbr, wcwidth, pyperclip, attrs, colorama, cmd2, PyYAML, stevedore, PrettyTable, cliff, colorlog, optuna, sox, soupsieve, beautifulsoup4, bs4, pytz, pandas, urllib3, idna, chardet, certifi, requests, setuptools, llvmlite, numba, audioread, threadpoolctl, joblib, scikit-learn, decorator, resampy, pycparser, cffi, soundfile, appdirs, pooch, librosa, ds-ctcdecoder, astor, wheel, termcolor, protobuf, werkzeug, markdown, grpcio, tensorboard, gast, tensorflow-estimator, wrapt, cached-property, h5py, keras-applications, google-pasta, keras-preprocessing, opt-einsum, tensorflow, deepspeech-training\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: python-utils\n",
            "    Found existing installation: python-utils 2.5.6\n",
            "    Uninstalling python-utils-2.5.6:\n",
            "      Successfully uninstalled python-utils-2.5.6\n",
            "  Attempting uninstall: progressbar2\n",
            "    Found existing installation: progressbar2 3.38.0\n",
            "    Uninstalling progressbar2-3.38.0:\n",
            "      Successfully uninstalled progressbar2-3.38.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.4.1\n",
            "    Uninstalling zipp-3.4.1:\n",
            "      Successfully uninstalled zipp-3.4.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 3.10.1\n",
            "    Uninstalling importlib-metadata-3.10.1:\n",
            "      Successfully uninstalled importlib-metadata-3.10.1\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 1.0.0\n",
            "    Uninstalling greenlet-1.0.0:\n",
            "      Successfully uninstalled greenlet-1.0.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 1.4.7\n",
            "    Uninstalling SQLAlchemy-1.4.7:\n",
            "      Successfully uninstalled SQLAlchemy-1.4.7\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 20.9\n",
            "    Uninstalling packaging-20.9:\n",
            "      Successfully uninstalled packaging-20.9\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.1\n",
            "    Uninstalling python-dateutil-2.8.1:\n",
            "      Successfully uninstalled python-dateutil-2.8.1\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 1.1.1\n",
            "    Uninstalling MarkupSafe-1.1.1:\n",
            "      Successfully uninstalled MarkupSafe-1.1.1\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.5\n",
            "    Uninstalling wcwidth-0.2.5:\n",
            "      Successfully uninstalled wcwidth-0.2.5\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 20.3.0\n",
            "    Uninstalling attrs-20.3.0:\n",
            "      Successfully uninstalled attrs-20.3.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: PrettyTable\n",
            "    Found existing installation: prettytable 2.1.0\n",
            "    Uninstalling prettytable-2.1.0:\n",
            "      Successfully uninstalled prettytable-2.1.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Attempting uninstall: bs4\n",
            "    Found existing installation: bs4 0.0.1\n",
            "    Uninstalling bs4-0.0.1:\n",
            "      Successfully uninstalled bs4-0.0.1\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2020.12.5\n",
            "    Uninstalling certifi-2020.12.5:\n",
            "      Successfully uninstalled certifi-2020.12.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 46.1.3\n",
            "    Uninstalling setuptools-46.1.3:\n",
            "      Successfully uninstalled setuptools-46.1.3\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: audioread\n",
            "    Found existing installation: audioread 2.1.9\n",
            "    Uninstalling audioread-2.1.9:\n",
            "      Successfully uninstalled audioread-2.1.9\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.2.2\n",
            "    Uninstalling resampy-0.2.2:\n",
            "      Successfully uninstalled resampy-0.2.2\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.20\n",
            "    Uninstalling pycparser-2.20:\n",
            "      Successfully uninstalled pycparser-2.20\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.14.5\n",
            "    Uninstalling cffi-1.14.5:\n",
            "      Successfully uninstalled cffi-1.14.5\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: SoundFile 0.10.3.post1\n",
            "    Uninstalling SoundFile-0.10.3.post1:\n",
            "      Successfully uninstalled SoundFile-0.10.3.post1\n",
            "  Attempting uninstall: appdirs\n",
            "    Found existing installation: appdirs 1.4.4\n",
            "    Uninstalling appdirs-1.4.4:\n",
            "      Successfully uninstalled appdirs-1.4.4\n",
            "  Attempting uninstall: pooch\n",
            "    Found existing installation: pooch 1.3.0\n",
            "    Uninstalling pooch-1.3.0:\n",
            "      Successfully uninstalled pooch-1.3.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.0\n",
            "    Uninstalling librosa-0.8.0:\n",
            "      Successfully uninstalled librosa-0.8.0\n",
            "  Attempting uninstall: astor\n",
            "    Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.34.2\n",
            "    Uninstalling wheel-0.34.2:\n",
            "      Successfully uninstalled wheel-0.34.2\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.3.4\n",
            "    Uninstalling Markdown-3.3.4:\n",
            "      Successfully uninstalled Markdown-3.3.4\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: keras-preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Running setup.py develop for deepspeech-training\n",
            "Successfully installed Mako-1.1.4 MarkupSafe-1.1.1 PrettyTable-2.1.0 PyYAML-5.4.1 absl-py-0.12.0 alembic-1.5.8 appdirs-1.4.4 astor-0.8.1 attrdict-2.0.1 attrs-20.3.0 audioread-2.1.9 beautifulsoup4-4.9.3 bs4-0.0.1 cached-property-1.5.2 certifi-2020.12.5 cffi-1.14.5 chardet-4.0.0 cliff-3.7.0 cmaes-0.8.2 cmd2-1.5.0 colorama-0.4.4 colorlog-5.0.1 decorator-5.0.7 deepspeech-training ds-ctcdecoder-0.7.4 gast-0.2.2 google-pasta-0.2.0 greenlet-1.0.0 grpcio-1.37.0 h5py-3.2.1 idna-2.10 importlib-metadata-4.0.1 joblib-1.0.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 librosa-0.8.0 llvmlite-0.31.0 markdown-3.3.4 numba-0.47.0 numpy-1.20.2 opt-einsum-3.3.0 optuna-2.7.0 opuslib-2.0.0 packaging-20.9 pandas-1.2.4 pbr-5.5.1 pooch-1.3.0 progressbar2-3.53.1 protobuf-3.15.8 pycparser-2.20 pyparsing-2.4.7 pyperclip-1.8.2 python-dateutil-2.8.1 python-editor-1.0.4 python-utils-2.5.6 pytz-2021.1 pyxdg-0.27 requests-2.25.1 resampy-0.2.2 scikit-learn-0.24.1 scipy-1.6.3 semver-2.13.0 setuptools-56.0.0 six-1.15.0 soundfile-0.10.3.post1 soupsieve-2.2.1 sox-1.4.1 sqlalchemy-1.4.11 stevedore-3.3.0 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1 termcolor-1.1.0 threadpoolctl-2.1.0 tqdm-4.60.0 typing-extensions-3.7.4.3 urllib3-1.26.4 wcwidth-0.2.5 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "astor",
                  "cffi",
                  "dateutil",
                  "decorator",
                  "google",
                  "numpy",
                  "pandas",
                  "pkg_resources",
                  "pyparsing",
                  "pytz",
                  "six",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists: /content/drive/My Drive/deepspeech/tc/native_client.tar.xz\n",
            "libdeepspeech.so\n",
            "LICENSE\n",
            "deepspeech\n",
            "deepspeech.h\n",
            "README.mozilla\n",
            "Found existing installation: protobuf 3.15.8\n",
            "Uninstalling protobuf-3.15.8:\n",
            "  Successfully uninstalled protobuf-3.15.8\n",
            "Collecting protobuf==3.8\n",
            "  Downloading protobuf-3.8.0-cp37-cp37m-manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf==3.8) (56.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf==3.8) (1.15.0)\n",
            "\u001b[31mERROR: googleapis-common-protos 1.53.0 has requirement protobuf>=3.12.0, but you'll have protobuf 3.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement protobuf>=3.12.0, but you'll have protobuf 3.8.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: protobuf\n",
            "Successfully installed protobuf-3.8.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 1.15.2\n",
            "Uninstalling tensorflow-1.15.2:\n",
            "  Successfully uninstalled tensorflow-1.15.2\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "TensorFlow 1.x selected.\n",
            "Collecting tensorflow-gpu==1.15.2\n",
            "  Downloading tensorflow_gpu-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (410.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 410.9 MB 34 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.20.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.37.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow-gpu==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (56.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (4.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.2) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.2) (3.7.4.3)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.15.2\n",
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CGCMxT64k-6",
        "outputId": "5b212a92-0114-4f83-c652-776f68c3ce71"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfVffGOO4mUw"
      },
      "source": [
        "!chmod a+x '/content/drive/My Drive/kenlm/build/bin/lmplz'\n",
        "\n",
        "!chmod a+x '/content/drive/My Drive/kenlm/build/bin/build_binary'\n",
        "\n",
        "!chmod a+x '/content/drive/My Drive/kenlm/build/bin/filter'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThC-ljbBhdqs"
      },
      "source": [
        "## Transfer Learning Speaker1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccekoJ96naRO",
        "outputId": "cb0783b1-8857-4372-e283-ae4f7b8459a0"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "\n",
        "! python3 generate_lm.py --input_txt '/content/drive/MyDrive/deepspeech/data/TransferLearning/vocab.txt' --output_dir . \\\n",
        "  --top_k 10 --kenlm_bins '/content/drive/My Drive/kenlm/build/bin' \\\n",
        "  --arpa_order 3 --max_arpa_memory \"85%\" --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 --binary_q_bits 8 --binary_type trie --discount_fallback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |#                                                   | 9 Elapsed Time: 0:00:00\n",
            "\n",
            "Saving top 10 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 10 words in total\n",
            "  It has 10 unique words\n",
            "  Your top-10 words are 100.0000 percent of all words\n",
            "  Your most common word \"இரண்டு\" occurred 1 times\n",
            "  The least common word in your top-k is \"ஒன்பது\" with 1 times\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/drive/My Drive/deepspeech/data/lm/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2672394240 bytes == 0x559d89258000 @  0x7fa8302b21e7 0x559d87726772 0x559d876ba358 0x559d87699290 0x559d87685096 0x7fa82e44bbf7 0x559d87686ada\n",
            "tcmalloc: large alloc 8907980800 bytes == 0x559e286f0000 @  0x7fa8302b21e7 0x559d87726772 0x559d877107aa 0x559d877111c8 0x559d876992ad 0x559d87685096 0x7fa82e44bbf7 0x559d87686ada\n",
            "****************************************************************************************************\n",
            "Unigram tokens 10 types 13\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:156 2:4036708864 3:7568829440\n",
            "tcmalloc: large alloc 7568834560 bytes == 0x559d8914a000 @  0x7fa8302b21e7 0x559d87726772 0x559d877107aa 0x559d877111c8 0x559d8769984e 0x559d87685096 0x7fa82e44bbf7 0x559d87686ada\n",
            "tcmalloc: large alloc 4036714496 bytes == 0x55a03becc000 @  0x7fa8302b21e7 0x559d87726772 0x559d877107aa 0x559d877111c8 0x559d87699c3d 0x559d87685096 0x7fa82e44bbf7 0x559d87686ada\n",
            "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
            "Statistics:\n",
            "1 13 D1=0.5 D2=1 D3+=1.5\n",
            "2 19 D1=0.5 D2=1 D3+=1.5\n",
            "3 0/9 D1=0.5 D2=1 D3+=1.5\n",
            "Memory estimate for binary LM:\n",
            "type       B\n",
            "probing  808 assuming -p 1.5\n",
            "probing  976 assuming -r models -p 1.5\n",
            "trie     541 without quantization\n",
            "trie    3500 assuming -q 8 -b 8 quantization \n",
            "trie     564 assuming -a 22 array pointer compression\n",
            "trie    3523 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "Name:lmplz\tVmPeak:15538396 kB\tVmRSS:2632544 kB\tRSSMax:2640324 kB\tuser:0.217692\tsys:1.19831\tCPU:1.41604\treal:1.50382\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading ./lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading ./lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqyPi4EQnaUg",
        "outputId": "0de50b9f-70be-4c7b-ec8a-cd10c93ed50b"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "! python3 ./generate_package.py --alphabet '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' --lm lm.binary --vocab '/content/drive/My Drive/deepspeech/data/lm/vocab-10.txt' \\\n",
        "  --package kenlm.scorer --default_alpha 0.931289039105002 --default_beta 1.1834137581510284"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "10 unique words read from vocabulary file.\n",
            "Doesn't look like a character based model.\n",
            "Using detected UTF-8 mode: False\n",
            "Package created in kenlm.scorer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn2ZGQcenOfC",
        "outputId": "8b190827-e645-4165-a8d5-eabbe578241f"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech'\n",
        "\n",
        "! python3 DeepSpeech.py \\\n",
        "    --drop_source_layers 1 \\\n",
        "    --alphabet_config_path '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' \\\n",
        "    --save_checkpoint_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Checkpoints/Speaker1Test' \\\n",
        "    --load_checkpoint_dir '/content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints' \\\n",
        "    --test_output_file '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Output/Speaker1Test/output.json' \\\n",
        "    --feature_cache '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/FeatureCache/Speaker1Test/feature_cache_' \\\n",
        "    --scorer '/content/drive/My Drive/deepspeech/data/lm/kenlm.scorer' \\\n",
        "    --train_files   '/content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTrain.csv' \\\n",
        "    --test_files  '/content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTest.csv' \\\n",
        "    --learning_rate 0.0001 \\\n",
        "    --train_batch_size 64 \\\n",
        "    --test_batch_size 1 \\\n",
        "    --export_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Export/Speaker1Test' \\\n",
        "    --summary_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Summary/Speaker1Test' \\\n",
        "    --n_hidden 1024 \\\n",
        "    --dropout_rate 0.4 \\\n",
        "    --epochs 10 \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech\n",
            "W WARNING: You specified different values for --load_checkpoint_dir and --save_checkpoint_dir, but you are running training and testing in a single invocation. The testing step will respect --load_checkpoint_dir, and thus WILL NOT TEST THE CHECKPOINT CREATED BY THE TRAINING STEP. Train and test in two separate invocations, specifying the correct --load_checkpoint_dir in both cases, or use the same location for loading and saving.\n",
            "I0425 17:17:13.090116 139752511256448 utils.py:157] NumExpr defaulting to 2 threads.\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I Initializing variable: layer_6/bias\n",
            "I Initializing variable: layer_6/bias/Adam\n",
            "I Initializing variable: layer_6/bias/Adam_1\n",
            "I Initializing variable: layer_6/weights\n",
            "I Initializing variable: layer_6/weights/Adam\n",
            "I Initializing variable: layer_6/weights/Adam_1\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:02 | Steps: 7 | Loss: 180.379719      \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 47.261767       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 21.144004       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 16.894511       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 15.560805       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 14.307791       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 13.416455       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.947127       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.516068       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.160443       \n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:00:24.089074\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTest.csv\n",
            "Test epoch | Steps: 40 | Elapsed Time: 0:00:07                                  \n",
            "Test on /content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTest.csv - WER: 0.075000, CER: 0.126168, loss: 7.076833\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 28.759224\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/110.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 13.732001\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/340.wav\n",
            " - src: \"இரண்டு\"\n",
            " - res: \"இரண்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 9.264908\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/119.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 8.970292\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/74.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 8.456483\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/2.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு \"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 5.433965\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/366.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.865780\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/352.wav\n",
            " - src: \"ஐந்து\"\n",
            " - res: \"ஐந்து\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 4.524983\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/539.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.355061\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/285.wav\n",
            " - src: \"இரண்டு\"\n",
            " - res: \"இரண்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.237683\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/437.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"ஏழு\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 0.924031\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/50.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"மூன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 0.923165\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/168.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"மூன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 34.589481\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/261.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"ஒன்று \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 27.426764\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/396.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 7.825887\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/288.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "I Exporting the model...\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Models exported at /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Export/Speaker1Test\n",
            "I Model metadata file saved to /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Export/Speaker1Test/author_model_0.0.1.md. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD-cVjhB_Gsc",
        "outputId": "d7505d97-171d-4875-d16c-f20aedce18e9"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech'\n",
        "\n",
        "! python3 DeepSpeech.py \\\n",
        "    --drop_source_layers 1 \\\n",
        "    --alphabet_config_path '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' \\\n",
        "    --save_checkpoint_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Checkpoints/Speaker1Train' \\\n",
        "    --load_checkpoint_dir '/content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints' \\\n",
        "    --test_output_file '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Output/Speaker1Train/output.json' \\\n",
        "    --feature_cache '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/FeatureCache/Speaker1Train/feature_cache_' \\\n",
        "    --scorer '/content/drive/My Drive/deepspeech/data/lm/kenlm.scorer' \\\n",
        "    --train_files   '/content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTrain.csv' \\\n",
        "    --test_files  '/content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTrain.csv' \\\n",
        "    --learning_rate 0.0001 \\\n",
        "    --train_batch_size 64 \\\n",
        "    --test_batch_size 64 \\\n",
        "    --export_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Export/Speaker1Train' \\\n",
        "    --summary_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Summary/Speaker1Train' \\\n",
        "    --n_hidden 1024 \\\n",
        "    --dropout_rate 0.4 \\\n",
        "    --epochs 10 \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech\n",
            "W WARNING: You specified different values for --load_checkpoint_dir and --save_checkpoint_dir, but you are running training and testing in a single invocation. The testing step will respect --load_checkpoint_dir, and thus WILL NOT TEST THE CHECKPOINT CREATED BY THE TRAINING STEP. Train and test in two separate invocations, specifying the correct --load_checkpoint_dir in both cases, or use the same location for loading and saving.\n",
            "I0425 17:18:18.818438 139992591939456 utils.py:157] NumExpr defaulting to 2 threads.\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I Initializing variable: layer_6/bias\n",
            "I Initializing variable: layer_6/bias/Adam\n",
            "I Initializing variable: layer_6/bias/Adam_1\n",
            "I Initializing variable: layer_6/weights\n",
            "I Initializing variable: layer_6/weights/Adam\n",
            "I Initializing variable: layer_6/weights/Adam_1\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:03 | Steps: 7 | Loss: 180.379803      \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 47.261712       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 21.143959       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 16.894462       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 15.560775       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 14.307756       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 13.416445       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.947174       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.516047       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.160406       \n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:00:24.194892\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTrain.csv\n",
            "Test epoch | Steps: 8 | Elapsed Time: 0:00:55                                   \n",
            "Test on /content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTrain.csv - WER: 0.136000, CER: 0.195307, loss: 8.573269\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.142857, loss: 32.278156\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/403.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம் \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 28.151432\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/260.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.142857, loss: 25.646893\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/293.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம் \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 25.312111\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/54.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.142857, loss: 25.202909\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/5.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம் \"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.800704\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/338.wav\n",
            " - src: \"ஒன்று\"\n",
            " - res: \"ஒன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.250000, loss: 4.761668\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/381.wav\n",
            " - src: \"ஆறு \"\n",
            " - res: \"ஆறு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.752374\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/49.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"மூன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.745154\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/461.wav\n",
            " - src: \"ஒன்று\"\n",
            " - res: \"ஒன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.735702\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/475.wav\n",
            " - src: \"ஒன்று\"\n",
            " - res: \"ஒன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 4.513894\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/344.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 4.411370\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/27.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 4.055587\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/135.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 3.634001\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/164.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 3.466207\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/104.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "I Exporting the model...\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Models exported at /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Export/Speaker1Train\n",
            "I Model metadata file saved to /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Export/Speaker1Train/author_model_0.0.1.md. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ0b2lnH6wAz"
      },
      "source": [
        "## Transfer Learning Speaker2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GoYcsfN6zi6",
        "outputId": "7efd9b73-acf0-49bc-e9b2-fd899503cbb5"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "\n",
        "! python3 generate_lm.py --input_txt '/content/drive/MyDrive/deepspeech/data/TransferLearning/vocab.txt' --output_dir . \\\n",
        "  --top_k 10 --kenlm_bins '/content/drive/My Drive/kenlm/build/bin' \\\n",
        "  --arpa_order 3 --max_arpa_memory \"85%\" --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 --binary_q_bits 8 --binary_type trie --discount_fallback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |#                                                   | 9 Elapsed Time: 0:00:00\n",
            "\n",
            "Saving top 10 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 10 words in total\n",
            "  It has 10 unique words\n",
            "  Your top-10 words are 100.0000 percent of all words\n",
            "  Your most common word \"இரண்டு\" occurred 1 times\n",
            "  The least common word in your top-k is \"ஒன்பது\" with 1 times\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/drive/My Drive/deepspeech/data/lm/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2672394240 bytes == 0x557503fc8000 @  0x7f94a1d081e7 0x557502f8d772 0x557502f21358 0x557502f00290 0x557502eec096 0x7f949fea1bf7 0x557502eedada\n",
            "tcmalloc: large alloc 8907980800 bytes == 0x5575a3460000 @  0x7f94a1d081e7 0x557502f8d772 0x557502f777aa 0x557502f781c8 0x557502f002ad 0x557502eec096 0x7f949fea1bf7 0x557502eedada\n",
            "****************************************************************************************************\n",
            "Unigram tokens 10 types 13\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:156 2:4036708864 3:7568829440\n",
            "tcmalloc: large alloc 7568834560 bytes == 0x557503eba000 @  0x7f94a1d081e7 0x557502f8d772 0x557502f777aa 0x557502f781c8 0x557502f0084e 0x557502eec096 0x7f949fea1bf7 0x557502eedada\n",
            "tcmalloc: large alloc 4036714496 bytes == 0x5577b6c3c000 @  0x7f94a1d081e7 0x557502f8d772 0x557502f777aa 0x557502f781c8 0x557502f00c3d 0x557502eec096 0x7f949fea1bf7 0x557502eedada\n",
            "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
            "Statistics:\n",
            "1 13 D1=0.5 D2=1 D3+=1.5\n",
            "2 19 D1=0.5 D2=1 D3+=1.5\n",
            "3 0/9 D1=0.5 D2=1 D3+=1.5\n",
            "Memory estimate for binary LM:\n",
            "type       B\n",
            "probing  808 assuming -p 1.5\n",
            "probing  976 assuming -r models -p 1.5\n",
            "trie     541 without quantization\n",
            "trie    3500 assuming -q 8 -b 8 quantization \n",
            "trie     564 assuming -a 22 array pointer compression\n",
            "trie    3523 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "Name:lmplz\tVmPeak:15538396 kB\tVmRSS:2632772 kB\tRSSMax:2644448 kB\tuser:0.220711\tsys:1.11473\tCPU:1.33548\treal:1.47648\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading ./lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading ./lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V9Bf_e06zoL",
        "outputId": "8720b68d-6892-4c91-fa67-bbdc4043a254"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "! python3 ./generate_package.py --alphabet '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' --lm lm.binary --vocab '/content/drive/My Drive/deepspeech/data/lm/vocab-10.txt' \\\n",
        "  --package kenlm.scorer --default_alpha 0.931289039105002 --default_beta 1.1834137581510284"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "10 unique words read from vocabulary file.\n",
            "Doesn't look like a character based model.\n",
            "Using detected UTF-8 mode: False\n",
            "Package created in kenlm.scorer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PI84FI76zrx",
        "outputId": "97c4929f-7165-4592-ff91-d6b3e1f733d7"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech'\n",
        "\n",
        "! python3 DeepSpeech.py \\\n",
        "    --drop_source_layers 1 \\\n",
        "    --alphabet_config_path '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' \\\n",
        "    --save_checkpoint_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2/Checkpoints' \\\n",
        "    --load_checkpoint_dir '/content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints' \\\n",
        "    --test_output_file '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2/Output/output.json' \\\n",
        "    --feature_cache '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2/FeatureCache/feature_cache_' \\\n",
        "    --scorer '/content/drive/My Drive/deepspeech/data/lm/kenlm.scorer' \\\n",
        "    --train_files   '/content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTrain.csv' \\\n",
        "    --test_files  '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2.csv' \\\n",
        "    --learning_rate 0.0001 \\\n",
        "    --train_batch_size 64 \\\n",
        "    --test_batch_size 1 \\\n",
        "    --export_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2/Export' \\\n",
        "    --summary_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2/Summary' \\\n",
        "    --n_hidden 1024 \\\n",
        "    --dropout_rate 0.4 \\\n",
        "    --epochs 10 \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech\n",
            "W WARNING: You specified different values for --load_checkpoint_dir and --save_checkpoint_dir, but you are running training and testing in a single invocation. The testing step will respect --load_checkpoint_dir, and thus WILL NOT TEST THE CHECKPOINT CREATED BY THE TRAINING STEP. Train and test in two separate invocations, specifying the correct --load_checkpoint_dir in both cases, or use the same location for loading and saving.\n",
            "I0425 17:23:54.557508 140564880369536 utils.py:157] NumExpr defaulting to 2 threads.\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I Initializing variable: layer_6/bias\n",
            "I Initializing variable: layer_6/bias/Adam\n",
            "I Initializing variable: layer_6/bias/Adam_1\n",
            "I Initializing variable: layer_6/weights\n",
            "I Initializing variable: layer_6/weights/Adam\n",
            "I Initializing variable: layer_6/weights/Adam_1\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:03 | Steps: 7 | Loss: 180.379782      \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 47.261774       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 21.144023       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 16.894465       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 15.560764       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 14.307760       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 13.416378       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.947198       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.516047       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.160319       \n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:00:24.603439\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2.csv\n",
            "Test epoch | Steps: 60 | Elapsed Time: 0:00:19                                  \n",
            "Test on /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2.csv - WER: 0.233333, CER: 0.172840, loss: 13.754828\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 30.254839\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/55.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 25.905611\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/51.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 24.056938\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/54.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 22.334839\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/53.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 21.718853\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/52.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.250000, loss: 10.560556\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/5.wav\n",
            " - src: \"ஆறு \"\n",
            " - res: \"ஆறு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 10.468583\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/9.wav\n",
            " - src: \"ஐந்து\"\n",
            " - res: \"ஐந்து\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 10.280015\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/27.wav\n",
            " - src: \"ஐந்து\"\n",
            " - res: \"ஐந்து\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 10.270951\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/14.wav\n",
            " - src: \"ஐந்து\"\n",
            " - res: \"ஐந்து\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 10.000226\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/47.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 10.366676\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/26.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 8.206315\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/8.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 7.847466\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/3.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 7.279685\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/28.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 6.453996\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/11.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "I Exporting the model...\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Models exported at /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2/Export\n",
            "I Model metadata file saved to /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2/Export/author_model_0.0.1.md. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZWWRBIj6wem"
      },
      "source": [
        "## Transfer Learning Speaker3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gbpSqf360M7",
        "outputId": "8c049e57-d42a-4ce5-8f86-669b862f486d"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "\n",
        "! python3 generate_lm.py --input_txt '/content/drive/MyDrive/deepspeech/data/TransferLearning/vocab.txt' --output_dir . \\\n",
        "  --top_k 10 --kenlm_bins '/content/drive/My Drive/kenlm/build/bin' \\\n",
        "  --arpa_order 3 --max_arpa_memory \"85%\" --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 --binary_q_bits 8 --binary_type trie --discount_fallback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |#                                                   | 9 Elapsed Time: 0:00:00\n",
            "\n",
            "Saving top 10 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 10 words in total\n",
            "  It has 10 unique words\n",
            "  Your top-10 words are 100.0000 percent of all words\n",
            "  Your most common word \"இரண்டு\" occurred 1 times\n",
            "  The least common word in your top-k is \"ஒன்பது\" with 1 times\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/drive/My Drive/deepspeech/data/lm/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2672394240 bytes == 0x556cfec18000 @  0x7f38c00651e7 0x556cfd02d772 0x556cfcfc1358 0x556cfcfa0290 0x556cfcf8c096 0x7f38be1febf7 0x556cfcf8dada\n",
            "tcmalloc: large alloc 8907980800 bytes == 0x556d9e0b0000 @  0x7f38c00651e7 0x556cfd02d772 0x556cfd0177aa 0x556cfd0181c8 0x556cfcfa02ad 0x556cfcf8c096 0x7f38be1febf7 0x556cfcf8dada\n",
            "****************************************************************************************************\n",
            "Unigram tokens 10 types 13\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:156 2:4036708864 3:7568829440\n",
            "tcmalloc: large alloc 7568834560 bytes == 0x556cfeb0a000 @  0x7f38c00651e7 0x556cfd02d772 0x556cfd0177aa 0x556cfd0181c8 0x556cfcfa084e 0x556cfcf8c096 0x7f38be1febf7 0x556cfcf8dada\n",
            "tcmalloc: large alloc 4036714496 bytes == 0x556fb188c000 @  0x7f38c00651e7 0x556cfd02d772 0x556cfd0177aa 0x556cfd0181c8 0x556cfcfa0c3d 0x556cfcf8c096 0x7f38be1febf7 0x556cfcf8dada\n",
            "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
            "Statistics:\n",
            "1 13 D1=0.5 D2=1 D3+=1.5\n",
            "2 19 D1=0.5 D2=1 D3+=1.5\n",
            "3 0/9 D1=0.5 D2=1 D3+=1.5\n",
            "Memory estimate for binary LM:\n",
            "type       B\n",
            "probing  808 assuming -p 1.5\n",
            "probing  976 assuming -r models -p 1.5\n",
            "trie     541 without quantization\n",
            "trie    3500 assuming -q 8 -b 8 quantization \n",
            "trie     564 assuming -a 22 array pointer compression\n",
            "trie    3523 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "Name:lmplz\tVmPeak:15538392 kB\tVmRSS:2632604 kB\tRSSMax:2644440 kB\tuser:0.187236\tsys:1.17549\tCPU:1.36275\treal:1.48089\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading ./lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading ./lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac7_9mAz60Qv",
        "outputId": "ffde7a2b-0171-4ec7-a468-7fb4b8cab8e0"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "! python3 ./generate_package.py --alphabet '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' --lm lm.binary --vocab '/content/drive/My Drive/deepspeech/data/lm/vocab-10.txt' \\\n",
        "  --package kenlm.scorer --default_alpha 0.931289039105002 --default_beta 1.1834137581510284"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "10 unique words read from vocabulary file.\n",
            "Doesn't look like a character based model.\n",
            "Using detected UTF-8 mode: False\n",
            "Package created in kenlm.scorer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VipRG3mG60T6",
        "outputId": "37d22ee6-2621-4d02-a00e-25d999f10941"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech'\n",
        "\n",
        "! python3 DeepSpeech.py \\\n",
        "    --drop_source_layers 1 \\\n",
        "    --alphabet_config_path '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' \\\n",
        "    --save_checkpoint_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3/Checkpoints' \\\n",
        "    --load_checkpoint_dir '/content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints' \\\n",
        "    --test_output_file '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3/Output/output.json' \\\n",
        "    --feature_cache '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3/FeatureCache/feature_cache_' \\\n",
        "    --scorer '/content/drive/My Drive/deepspeech/data/lm/kenlm.scorer' \\\n",
        "    --train_files   '/content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTrain.csv' \\\n",
        "    --test_files  '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3.csv' \\\n",
        "    --learning_rate 0.0001 \\\n",
        "    --train_batch_size 64 \\\n",
        "    --test_batch_size 1 \\\n",
        "    --export_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3/Export' \\\n",
        "    --summary_dir '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3/Summary' \\\n",
        "    --n_hidden 1024 \\\n",
        "    --dropout_rate 0.4 \\\n",
        "    --epochs 10 \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech\n",
            "W WARNING: You specified different values for --load_checkpoint_dir and --save_checkpoint_dir, but you are running training and testing in a single invocation. The testing step will respect --load_checkpoint_dir, and thus WILL NOT TEST THE CHECKPOINT CREATED BY THE TRAINING STEP. Train and test in two separate invocations, specifying the correct --load_checkpoint_dir in both cases, or use the same location for loading and saving.\n",
            "I0425 17:24:47.149665 140186486503296 utils.py:157] NumExpr defaulting to 2 threads.\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I Initializing variable: layer_6/bias\n",
            "I Initializing variable: layer_6/bias/Adam\n",
            "I Initializing variable: layer_6/bias/Adam_1\n",
            "I Initializing variable: layer_6/weights\n",
            "I Initializing variable: layer_6/weights/Adam\n",
            "I Initializing variable: layer_6/weights/Adam_1\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:00:03 | Steps: 7 | Loss: 180.379656      \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 47.261929       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 21.144000       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 16.894649       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 15.560948       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 14.307840       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 13.416461       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.947153       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.516126       \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 |   Training | Elapsed Time: 0:00:01 | Steps: 7 | Loss: 12.160547       \n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:00:24.705599\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3.csv\n",
            "Test epoch | Steps: 60 | Elapsed Time: 0:00:10                                  \n",
            "Test on /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3.csv - WER: 0.050000, CER: 0.108974, loss: 9.720225\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.142857, loss: 28.325100\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/11.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம் \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.200000, loss: 24.657738\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/28.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 24.045185\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/31.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 23.825850\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/24.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 19.682833\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/2.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 8.322341\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/6.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 8.164310\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/23.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"ஏழு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 7.571328\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/12.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 7.014627\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/9.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 6.663802\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/34.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 1.902745\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/30.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"மூன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.250000, loss: 1.564955\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/13.wav\n",
            " - src: \"ஆறு \"\n",
            " - res: \"ஆறு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 18.291607\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/43.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.333333, loss: 14.654126\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/42.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"ஒன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 6.909765\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/18.wav\n",
            " - src: \"ஆறு \"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "I Exporting the model...\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Models exported at /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3/Export\n",
            "I Model metadata file saved to /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3/Export/author_model_0.0.1.md. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btlwJs5VIwZk"
      },
      "source": [
        "## Digit Recognition Testing on Trained Model for Speaker1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_WW6ZpJJI0u",
        "outputId": "b75e0753-e650-4480-8452-9743350bf805"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "\n",
        "! python3 generate_lm.py --input_txt '/content/drive/MyDrive/deepspeech/data/TransferLearning/vocab.txt' --output_dir . \\\n",
        "  --top_k 10 --kenlm_bins '/content/drive/My Drive/kenlm/build/bin' \\\n",
        "  --arpa_order 3 --max_arpa_memory \"85%\" --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 --binary_q_bits 8 --binary_type trie --discount_fallback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |#                                                   | 9 Elapsed Time: 0:00:00\n",
            "\n",
            "Saving top 10 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 10 words in total\n",
            "  It has 10 unique words\n",
            "  Your top-10 words are 100.0000 percent of all words\n",
            "  Your most common word \"இரண்டு\" occurred 1 times\n",
            "  The least common word in your top-k is \"ஒன்பது\" with 1 times\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/drive/My Drive/deepspeech/data/lm/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2672394240 bytes == 0x55e88fc0a000 @  0x7ff5cb1861e7 0x55e88d53d772 0x55e88d4d1358 0x55e88d4b0290 0x55e88d49c096 0x7ff5c931fbf7 0x55e88d49dada\n",
            "tcmalloc: large alloc 8907980800 bytes == 0x55e92f0a2000 @  0x7ff5cb1861e7 0x55e88d53d772 0x55e88d5277aa 0x55e88d5281c8 0x55e88d4b02ad 0x55e88d49c096 0x7ff5c931fbf7 0x55e88d49dada\n",
            "****************************************************************************************************\n",
            "Unigram tokens 10 types 13\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:156 2:4036707328 3:7568826368\n",
            "tcmalloc: large alloc 7568826368 bytes == 0x55e88fafc000 @  0x7ff5cb1861e7 0x55e88d53d772 0x55e88d5277aa 0x55e88d5281c8 0x55e88d4b084e 0x55e88d49c096 0x7ff5c931fbf7 0x55e88d49dada\n",
            "tcmalloc: large alloc 4036714496 bytes == 0x55eb4287e000 @  0x7ff5cb1861e7 0x55e88d53d772 0x55e88d5277aa 0x55e88d5281c8 0x55e88d4b0c3d 0x55e88d49c096 0x7ff5c931fbf7 0x55e88d49dada\n",
            "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
            "Statistics:\n",
            "1 13 D1=0.5 D2=1 D3+=1.5\n",
            "2 19 D1=0.5 D2=1 D3+=1.5\n",
            "3 0/9 D1=0.5 D2=1 D3+=1.5\n",
            "Memory estimate for binary LM:\n",
            "type       B\n",
            "probing  808 assuming -p 1.5\n",
            "probing  976 assuming -r models -p 1.5\n",
            "trie     541 without quantization\n",
            "trie    3500 assuming -q 8 -b 8 quantization \n",
            "trie     564 assuming -a 22 array pointer compression\n",
            "trie    3523 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "Name:lmplz\tVmPeak:15538396 kB\tVmRSS:2632620 kB\tRSSMax:2644480 kB\tuser:0.195941\tsys:1.08911\tCPU:1.28509\treal:1.46686\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading ./lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading ./lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCdSh_5EKvuE",
        "outputId": "55a9cf32-5dc9-4808-87fa-8c471bba67e4"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "! python3 ./generate_package.py --alphabet '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' --lm lm.binary --vocab '/content/drive/My Drive/deepspeech/data/lm/vocab-10.txt' \\\n",
        "  --package kenlm.scorer --default_alpha 0.931289039105002 --default_beta 1.1834137581510284"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "10 unique words read from vocabulary file.\n",
            "Doesn't look like a character based model.\n",
            "Using detected UTF-8 mode: False\n",
            "Package created in kenlm.scorer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVUDVL8AKv0s",
        "outputId": "d5029b79-569e-48c0-cc5b-d8bd97e0c0b5"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech'\n",
        "\n",
        "!python evaluate.py \\\n",
        "  --test_files '/content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTest.csv' \\\n",
        "  --test_output_file '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker1/Output/Speaker1Test/trained_model_output.json' \\\n",
        "  --checkpoint_dir '/content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints' \\\n",
        "  --scorer '/content/drive/My Drive/deepspeech/data/lm/kenlm.scorer' \\\n",
        "  --alphabet_config_path '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' \\\n",
        "  --n_hidden 1024 \\\n",
        "  --test_batch_size 1 \\\n",
        "  \"$@\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech\n",
            "2021-04-26 08:00:00.525605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-04-26 08:00:00.525982: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d441153800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-04-26 08:00:00.526022: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-04-26 08:00:00.616595: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-26 08:00:00.808632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:00.809441: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d444a00540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-04-26 08:00:00.809496: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-04-26 08:00:00.810496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:00.811018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-26 08:00:00.821288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-04-26 08:00:01.058371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-04-26 08:00:01.180032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-26 08:00:01.216919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-26 08:00:01.462218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-26 08:00:01.513069: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-04-26 08:00:02.013740: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-26 08:00:02.013919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:02.014650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:02.015186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-04-26 08:00:02.018170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-04-26 08:00:02.020256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-26 08:00:02.020288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-04-26 08:00:02.020299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-04-26 08:00:02.021234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:02.021851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:02.022413: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-26 08:00:02.022466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0426 08:00:03.379594 140705698682752 utils.py:157] NumExpr defaulting to 2 threads.\n",
            "2021-04-26 08:00:05.172278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:05.172921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-26 08:00:05.173009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-04-26 08:00:05.173031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-04-26 08:00:05.173050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-26 08:00:05.173068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-26 08:00:05.173109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-26 08:00:05.173142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-04-26 08:00:05.173160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-26 08:00:05.173235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:05.173853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:05.174420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-04-26 08:00:05.174491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-26 08:00:05.174505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-04-26 08:00:05.174515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-04-26 08:00:05.174617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:05.175163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:00:05.175694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTest.csv\n",
            "Test epoch | Steps: 0 | Elapsed Time: 0:00:00                                   2021-04-26 08:00:15.606289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-04-26 08:00:17.685913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "Test epoch | Steps: 40 | Elapsed Time: 0:00:18                                  \n",
            "Test on /content/drive/MyDrive/deepspeech/data/TransferLearning/TransferLearningTest.csv - WER: 0.075000, CER: 0.126168, loss: 7.076834\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 28.759228\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/110.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 13.732001\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/340.wav\n",
            " - src: \"இரண்டு\"\n",
            " - res: \"இரண்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 9.264910\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/119.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 8.970295\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/74.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 8.456484\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/2.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு \"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 5.433967\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/366.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.865779\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/352.wav\n",
            " - src: \"ஐந்து\"\n",
            " - res: \"ஐந்து\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 4.524985\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/539.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.355059\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/285.wav\n",
            " - src: \"இரண்டு\"\n",
            " - res: \"இரண்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 4.237683\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/437.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"ஏழு\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 0.924030\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/50.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"மூன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 0.923165\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/168.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"மூன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 34.589481\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/261.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"ஒன்று \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 27.426764\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/396.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 7.825887\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/288.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJk0ARrxIwj-"
      },
      "source": [
        "## Digit Recognition Testing on Trained Model for Speaker2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-20e29gZMTeI",
        "outputId": "03894ee0-b807-43e7-895f-d378cdbb5226"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "\n",
        "! python3 generate_lm.py --input_txt '/content/drive/MyDrive/deepspeech/data/TransferLearning/vocab.txt' --output_dir . \\\n",
        "  --top_k 10 --kenlm_bins '/content/drive/My Drive/kenlm/build/bin' \\\n",
        "  --arpa_order 3 --max_arpa_memory \"85%\" --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 --binary_q_bits 8 --binary_type trie --discount_fallback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |#                                                   | 9 Elapsed Time: 0:00:00\n",
            "\n",
            "Saving top 10 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 10 words in total\n",
            "  It has 10 unique words\n",
            "  Your top-10 words are 100.0000 percent of all words\n",
            "  Your most common word \"இரண்டு\" occurred 1 times\n",
            "  The least common word in your top-k is \"ஒன்பது\" with 1 times\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/drive/My Drive/deepspeech/data/lm/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2672394240 bytes == 0x5571fa0ac000 @  0x7f93c7caf1e7 0x5571f7585772 0x5571f7519358 0x5571f74f8290 0x5571f74e4096 0x7f93c5e48bf7 0x5571f74e5ada\n",
            "tcmalloc: large alloc 8907980800 bytes == 0x557299544000 @  0x7f93c7caf1e7 0x5571f7585772 0x5571f756f7aa 0x5571f75701c8 0x5571f74f82ad 0x5571f74e4096 0x7f93c5e48bf7 0x5571f74e5ada\n",
            "****************************************************************************************************\n",
            "Unigram tokens 10 types 13\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:156 2:4036707328 3:7568826368\n",
            "tcmalloc: large alloc 7568826368 bytes == 0x5571f9f9e000 @  0x7f93c7caf1e7 0x5571f7585772 0x5571f756f7aa 0x5571f75701c8 0x5571f74f884e 0x5571f74e4096 0x7f93c5e48bf7 0x5571f74e5ada\n",
            "tcmalloc: large alloc 4036714496 bytes == 0x5574acd20000 @  0x7f93c7caf1e7 0x5571f7585772 0x5571f756f7aa 0x5571f75701c8 0x5571f74f8c3d 0x5571f74e4096 0x7f93c5e48bf7 0x5571f74e5ada\n",
            "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
            "Statistics:\n",
            "1 13 D1=0.5 D2=1 D3+=1.5\n",
            "2 19 D1=0.5 D2=1 D3+=1.5\n",
            "3 0/9 D1=0.5 D2=1 D3+=1.5\n",
            "Memory estimate for binary LM:\n",
            "type       B\n",
            "probing  808 assuming -p 1.5\n",
            "probing  976 assuming -r models -p 1.5\n",
            "trie     541 without quantization\n",
            "trie    3500 assuming -q 8 -b 8 quantization \n",
            "trie     564 assuming -a 22 array pointer compression\n",
            "trie    3523 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "Name:lmplz\tVmPeak:15538392 kB\tVmRSS:2632436 kB\tRSSMax:2644300 kB\tuser:0.240056\tsys:1.12326\tCPU:1.36336\treal:1.45377\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading ./lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading ./lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZes6jmXMTow",
        "outputId": "9015bfed-d380-4fba-a80d-cd5f825f76f9"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "! python3 ./generate_package.py --alphabet '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' --lm lm.binary --vocab '/content/drive/My Drive/deepspeech/data/lm/vocab-10.txt' \\\n",
        "  --package kenlm.scorer --default_alpha 0.931289039105002 --default_beta 1.1834137581510284"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "10 unique words read from vocabulary file.\n",
            "Doesn't look like a character based model.\n",
            "Using detected UTF-8 mode: False\n",
            "Package created in kenlm.scorer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLoE1WkjMTwo",
        "outputId": "f32a1320-1f85-4f76-d11f-b602e458f9d0"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech'\n",
        "\n",
        "!python evaluate.py \\\n",
        "  --test_files '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2.csv' \\\n",
        "  --test_output_file '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2/Output/trained_model_output.json' \\\n",
        "  --checkpoint_dir '/content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints' \\\n",
        "  --scorer '/content/drive/My Drive/deepspeech/data/lm/kenlm.scorer' \\\n",
        "  --alphabet_config_path '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' \\\n",
        "  --n_hidden 1024 \\\n",
        "  --test_batch_size 1 \\\n",
        "  \"$@\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech\n",
            "2021-04-26 08:01:43.344202: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-04-26 08:01:43.344438: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d0d2df9480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-04-26 08:01:43.344474: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-04-26 08:01:43.346112: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-26 08:01:43.483202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:43.483994: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d0d67f21c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-04-26 08:01:43.484027: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-04-26 08:01:43.484213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:43.484857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-26 08:01:43.485164: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-04-26 08:01:43.486647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-04-26 08:01:43.488188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-26 08:01:43.488589: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-26 08:01:43.490088: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-26 08:01:43.490740: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-04-26 08:01:43.493640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-26 08:01:43.493741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:43.494397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:43.494965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-04-26 08:01:43.495039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-04-26 08:01:43.496022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-26 08:01:43.496048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-04-26 08:01:43.496058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-04-26 08:01:43.496194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:43.496808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:43.497318: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-26 08:01:43.497372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0426 08:01:44.048209 140647167674240 utils.py:157] NumExpr defaulting to 2 threads.\n",
            "2021-04-26 08:01:44.939199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:44.939841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-26 08:01:44.939919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-04-26 08:01:44.939942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-04-26 08:01:44.939960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-26 08:01:44.939978: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-26 08:01:44.939999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-26 08:01:44.940018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-04-26 08:01:44.940036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-26 08:01:44.940109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:44.940689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:44.941179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-04-26 08:01:44.941225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-26 08:01:44.941239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-04-26 08:01:44.941248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-04-26 08:01:44.941344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:44.941905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:01:44.942429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2.csv\n",
            "Test epoch | Steps: 0 | Elapsed Time: 0:00:00                                   2021-04-26 08:01:45.837667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-04-26 08:01:46.776458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-26 08:01:47.499201: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "Test epoch | Steps: 6 | Elapsed Time: 0:00:03                                   2021-04-26 08:01:49.377027: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "Test epoch | Steps: 12 | Elapsed Time: 0:00:06                                  2021-04-26 08:01:52.072315: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "Test epoch | Steps: 17 | Elapsed Time: 0:00:07                                  2021-04-26 08:01:53.616609: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "Test epoch | Steps: 25 | Elapsed Time: 0:00:10                                  2021-04-26 08:01:55.988840: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "Test epoch | Steps: 35 | Elapsed Time: 0:00:13                                  2021-04-26 08:01:59.463335: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "Test epoch | Steps: 39 | Elapsed Time: 0:00:14                                  2021-04-26 08:02:01.129548: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "2021-04-26 08:02:01.655387: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "Test epoch | Steps: 42 | Elapsed Time: 0:00:19                                  2021-04-26 08:02:05.576055: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "Test epoch | Steps: 52 | Elapsed Time: 0:00:22                                  2021-04-26 08:02:08.392774: E tensorflow/core/kernels/mfcc_mel_filterbank.cc:153] Missing 2 bands  starting at 1 in mel-frequency design. Perhaps too many channels or not enough frequency resolution in spectrum. (input_length: 257 input_sample_rate: 44100 output_channel_count: 40 lower_frequency_limit: 20 upper_frequency_limit: 8000\n",
            "Test epoch | Steps: 60 | Elapsed Time: 0:00:25                                  \n",
            "Test on /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker2.csv - WER: 0.233333, CER: 0.172840, loss: 13.754828\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 30.254843\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/55.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 25.905613\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/51.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 24.056940\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/54.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 22.334835\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/53.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 21.718863\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/52.wav\n",
            " - src: \"ஒன்பது\"\n",
            " - res: \"ஒன்பது\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.250000, loss: 10.560557\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/5.wav\n",
            " - src: \"ஆறு \"\n",
            " - res: \"ஆறு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 10.468583\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/9.wav\n",
            " - src: \"ஐந்து\"\n",
            " - res: \"ஐந்து\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 10.280014\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/27.wav\n",
            " - src: \"ஐந்து\"\n",
            " - res: \"ஐந்து\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 10.270948\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/14.wav\n",
            " - src: \"ஐந்து\"\n",
            " - res: \"ஐந்து\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 10.000226\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/47.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 10.366678\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/26.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 8.206316\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/8.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 7.847467\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/3.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 7.279683\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/28.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 6.453996\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker2/11.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUwaPqGfIwwD"
      },
      "source": [
        "## Digit Recognition Testing on Trained Model for Speaker3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u80AbQtPMx9H",
        "outputId": "23a1c019-c94a-436f-e06c-48a143a824e2"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "\n",
        "! python3 generate_lm.py --input_txt '/content/drive/MyDrive/deepspeech/data/TransferLearning/vocab.txt' --output_dir . \\\n",
        "  --top_k 10 --kenlm_bins '/content/drive/My Drive/kenlm/build/bin' \\\n",
        "  --arpa_order 3 --max_arpa_memory \"85%\" --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 --binary_q_bits 8 --binary_type trie --discount_fallback"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |#                                                   | 9 Elapsed Time: 0:00:00\n",
            "\n",
            "Saving top 10 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 10 words in total\n",
            "  It has 10 unique words\n",
            "  Your top-10 words are 100.0000 percent of all words\n",
            "  Your most common word \"இரண்டு\" occurred 1 times\n",
            "  The least common word in your top-k is \"ஒன்பது\" with 1 times\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/drive/My Drive/deepspeech/data/lm/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2672394240 bytes == 0x565430f1a000 @  0x7ff69c2831e7 0x56542f988772 0x56542f91c358 0x56542f8fb290 0x56542f8e7096 0x7ff69a41cbf7 0x56542f8e8ada\n",
            "tcmalloc: large alloc 8907980800 bytes == 0x5654d03b2000 @  0x7ff69c2831e7 0x56542f988772 0x56542f9727aa 0x56542f9731c8 0x56542f8fb2ad 0x56542f8e7096 0x7ff69a41cbf7 0x56542f8e8ada\n",
            "****************************************************************************************************\n",
            "Unigram tokens 10 types 13\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:156 2:4036707328 3:7568826368\n",
            "tcmalloc: large alloc 7568826368 bytes == 0x565430e0c000 @  0x7ff69c2831e7 0x56542f988772 0x56542f9727aa 0x56542f9731c8 0x56542f8fb84e 0x56542f8e7096 0x7ff69a41cbf7 0x56542f8e8ada\n",
            "tcmalloc: large alloc 4036714496 bytes == 0x5656e3b8e000 @  0x7ff69c2831e7 0x56542f988772 0x56542f9727aa 0x56542f9731c8 0x56542f8fbc3d 0x56542f8e7096 0x7ff69a41cbf7 0x56542f8e8ada\n",
            "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
            "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
            "Statistics:\n",
            "1 13 D1=0.5 D2=1 D3+=1.5\n",
            "2 19 D1=0.5 D2=1 D3+=1.5\n",
            "3 0/9 D1=0.5 D2=1 D3+=1.5\n",
            "Memory estimate for binary LM:\n",
            "type       B\n",
            "probing  808 assuming -p 1.5\n",
            "probing  976 assuming -r models -p 1.5\n",
            "trie     541 without quantization\n",
            "trie    3500 assuming -q 8 -b 8 quantization \n",
            "trie     564 assuming -a 22 array pointer compression\n",
            "trie    3523 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:156 2:304 3:20\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "\n",
            "Name:lmplz\tVmPeak:15538396 kB\tVmRSS:2632348 kB\tRSSMax:2633992 kB\tuser:0.186921\tsys:1.07855\tCPU:1.26551\treal:1.40528\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading ./lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading ./lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz3-YCjVMyes",
        "outputId": "59a67cb0-f318-4379-f4f3-07142da8034f"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech/data/lm'\n",
        "! python3 ./generate_package.py --alphabet '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' --lm lm.binary --vocab '/content/drive/My Drive/deepspeech/data/lm/vocab-10.txt' \\\n",
        "  --package kenlm.scorer --default_alpha 0.931289039105002 --default_beta 1.1834137581510284"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech/data/lm\n",
            "10 unique words read from vocabulary file.\n",
            "Doesn't look like a character based model.\n",
            "Using detected UTF-8 mode: False\n",
            "Package created in kenlm.scorer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD4q-P4CMyvB",
        "outputId": "ec0f60f6-2bf1-46e8-a0a5-44cad57bb4fe"
      },
      "source": [
        "%cd '/content/drive/My Drive/deepspeech'\n",
        "\n",
        "!python evaluate.py \\\n",
        "  --test_files '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3.csv' \\\n",
        "  --test_output_file '/content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3/Output/trained_model_output.json' \\\n",
        "  --checkpoint_dir '/content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints' \\\n",
        "  --scorer '/content/drive/My Drive/deepspeech/data/lm/kenlm.scorer' \\\n",
        "  --alphabet_config_path '/content/drive/My Drive/deepspeech/data/tamilalphabet.txt' \\\n",
        "  --n_hidden 1024 \\\n",
        "  --test_batch_size 1 \\\n",
        "  \"$@\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/deepspeech\n",
            "2021-04-26 08:02:48.402712: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-04-26 08:02:48.402928: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bd60b07480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-04-26 08:02:48.402959: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-04-26 08:02:48.405691: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-26 08:02:48.534883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:48.535624: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bd645001c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-04-26 08:02:48.535667: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-04-26 08:02:48.535837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:48.536381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-26 08:02:48.536664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-04-26 08:02:48.538043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-04-26 08:02:48.539523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-26 08:02:48.539838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-26 08:02:48.541185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-26 08:02:48.541825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-04-26 08:02:48.544614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-26 08:02:48.544715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:48.545270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:48.545784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-04-26 08:02:48.545840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-04-26 08:02:48.546827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-26 08:02:48.546853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-04-26 08:02:48.546864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-04-26 08:02:48.546971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:48.547612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:48.548146: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-26 08:02:48.548189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0426 08:02:49.086808 140476820793216 utils.py:157] NumExpr defaulting to 2 threads.\n",
            "2021-04-26 08:02:49.953418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:49.954027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-26 08:02:49.954098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-04-26 08:02:49.954121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-04-26 08:02:49.954142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-26 08:02:49.954162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-26 08:02:49.954185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-26 08:02:49.954203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-04-26 08:02:49.954222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-26 08:02:49.954299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:49.954887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:49.955394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-04-26 08:02:49.955441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-26 08:02:49.955456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-04-26 08:02:49.955466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-04-26 08:02:49.955588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:49.956136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-26 08:02:49.956666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I Loading best validating checkpoint from /content/drive/MyDrive/deepspeech/data/CommonVoiceTamilAndOpenSLR/Checkpoints/best_dev-6090\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3.csv\n",
            "Test epoch | Steps: 0 | Elapsed Time: 0:00:00                                   2021-04-26 08:02:50.843432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-04-26 08:02:51.653293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "Test epoch | Steps: 60 | Elapsed Time: 0:00:18                                  \n",
            "Test on /content/drive/MyDrive/deepspeech/data/TransferLearning/Speaker3.csv - WER: 0.050000, CER: 0.108974, loss: 9.720225\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.142857, loss: 28.325102\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/11.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம் \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.200000, loss: 24.657740\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/28.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 24.045181\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/31.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 23.825842\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/24.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 19.682833\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/2.wav\n",
            " - src: \"சுழியம்\"\n",
            " - res: \"சுழியம்\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 8.322342\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/6.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 8.164310\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/23.wav\n",
            " - src: \"ஏழு\"\n",
            " - res: \"ஏழு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 7.571324\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/12.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.166667, loss: 7.014630\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/9.wav\n",
            " - src: \"நான்கு\"\n",
            " - res: \"நான்கு \"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 6.663801\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/34.wav\n",
            " - src: \"எட்டு\"\n",
            " - res: \"எட்டு\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 1.902744\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/30.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"மூன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.250000, loss: 1.564953\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/13.wav\n",
            " - src: \"ஆறு \"\n",
            " - res: \"ஆறு\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 18.291616\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/43.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.333333, loss: 14.654125\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/42.wav\n",
            " - src: \"மூன்று\"\n",
            " - res: \"ஒன்று\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 1.000000, loss: 6.909764\n",
            " - wav: file:///content/drive/My Drive/deepspeech/data/TransferLearning/AudioFiles/Speaker3/18.wav\n",
            " - src: \"ஆறு \"\n",
            " - res: \"\"\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}